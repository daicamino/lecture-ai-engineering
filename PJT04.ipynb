{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daicamino/lecture-ai-engineering/blob/master/PJT04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JPT04 講義資料の質疑応答のまとめ機能の作成"
      ],
      "metadata": {
        "id": "ZbLTHAXjBgnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "神野大輔"
      ],
      "metadata": {
        "id": "vwZ8shQLj1V5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##はじめに"
      ],
      "metadata": {
        "id": "cLfJNVImBx9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "　本プログラムは、『講義内容質問フォーム（回答）』の質問を要約したうえで並び替え、講師が類似した質問をまとめて回答することを支援する質問要約ツールである。\n",
        "\n",
        "　生成モデルは、本講座の第３回演習のプログラムを引用しLlama3を利用している。また、それ以外のコードのドラフトはGeminiにより生成した。\n",
        "\n",
        "　「質問」または「要約」をもとに類似度によるグループ分けを試みたが、分類の精度が上げられなかった。また、要約をソートすることでもある程度視認性が改善できるため、質問の内容による分類は不採用とした。\n",
        "\n",
        "　加えて、処理中にも新しい質問が追加される可能性があるため、要約のソートは手動で行うこととする。\n",
        "\n",
        "　なお、モデルのダウンロードには時間を要するため、本プログラムの利用時には初期設定までは事前に完了していることが望ましい。\n",
        "\n"
      ],
      "metadata": {
        "id": "QhTAlw23B4Zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##取扱方法"
      ],
      "metadata": {
        "id": "91unLmPGE0Vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   hugging faceのトークンを準備ください。\n",
        "2.   『講義内容質問フォーム（回答）』のB列に「要約」を追加ください。もしも、プログラム実行時に該当の列がないばあいは、「要約」列が最後列に自動で追加されます。\n",
        "3.   『講義内容質問フォーム（回答）』を、Google スプレッドシート形式で保存してください。\n",
        "4.   このノートブックを「講義質問フォーム」と同じフォルダに格納\n",
        "してください。\n",
        "5.   処理完了後、『講義内容質問フォーム（回答）』を開き、手動にて「要約」列でソートしてください。"
      ],
      "metadata": {
        "id": "CBhZ6nTUE-Zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 初期設定"
      ],
      "metadata": {
        "id": "Q4Cc4njJf7lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##環境設定\n",
        "ご自身の環境に合わせて変更ください。"
      ],
      "metadata": {
        "id": "qohRnGBQgNky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV38lexbcnLR"
      },
      "outputs": [],
      "source": [
        "# 作業ディレクトリとQAファイル名の指定\n",
        "work_dir = '/content/drive/MyDrive/Colab Notebooks/lecture-ai-engineering/PJT04/'\n",
        "QAsheet = '講義内容質問フォーム（回答）'\n",
        "\n",
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goodle Driveへの接続と各種認証"
      ],
      "metadata": {
        "id": "FHntBGxhg2of"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6hqyDWKeMcl"
      },
      "outputs": [],
      "source": [
        "# Goodle Driveへの接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# colab Notebookの認証\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# gspreadの認証\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMモデルの読み込み"
      ],
      "metadata": {
        "id": "2qIFLy6cgJmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "　本講義の第３回演習のプログラムより引用。\n"
      ],
      "metadata": {
        "id": "w6pM3k3iG_xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "FbX_bDNIAIOH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI2_3He13Ozi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# モデル(Llama3)の読み込み\n",
        "\n",
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#処理"
      ],
      "metadata": {
        "id": "y6Ed0gm6uncc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GSpreadの読み込み"
      ],
      "metadata": {
        "id": "g3b_GX8HJAiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LPGDVrOjSyH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "worksheet = gc.open(QAsheet).sheet1\n",
        "rows = worksheet.get_all_values()\n",
        "qa_data = pd.DataFrame(rows[1:], columns=rows[0])\n",
        "\n",
        "# 「要約」列が存在しない場合は追加\n",
        "if '要約' not in qa_data.columns:\n",
        "    qa_data['要約'] = ''\n",
        "    # スプレッドシートにも列を追加 (ヘッダー行)\n",
        "    worksheet.insert_cols([['要約']], col=len(rows[0]) + 1)\n",
        "\n",
        "# prompt: qa_dataの内容を表示\n",
        "qa_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##質問の要約"
      ],
      "metadata": {
        "id": "sUklYk_BuxJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: もしも'要約'が空白のとき、各行の質問を要約して、ワークシートの'要約'列に挿入する。\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "# 各行の「要約」列が空白かどうかを確認し、空白の場合は要約を生成して挿入\n",
        "for index, row in qa_data.iterrows():\n",
        "    if row['要約'] == '':\n",
        "        question = row['質問']\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"コンテンツの件名を30文字以内で生成する。コンテンツに記載されていないことは表示しない。日本語で回答する。\"},\n",
        "            {\"role\": \"user\", \"content\": f\"コンテンツ：{question}\"},\n",
        "        ]\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        summary_output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=25, # 要約のトークン数\n",
        "            eos_token_id=terminators,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "        summary_tokens = summary_output[0][input_ids.shape[-1]:]\n",
        "        summary_str = tokenizer.decode(summary_tokens, skip_special_tokens=True)\n",
        "        summary_str = summary_str.replace('タイトル：', '').replace('件名：', '').replace('「', '').replace('」', '')\n",
        "\n",
        "        # DataFrameの「要約」列を更新\n",
        "        qa_data.at[index, '要約'] = summary_str\n",
        "\n",
        "        # スプレッドシートの該当セルを更新\n",
        "        # ヘッダー行があるため、DataFrameのインデックスに2を加える (0始まりのDataFrameインデックス + 1 (ヘッダー) + 1 (スプレッドシートは1始まり))\n",
        "        # 「要約」列のインデックスを取得\n",
        "        summary_col_index = qa_data.columns.get_loc('要約') + 1 # gspreadは1始まり\n",
        "\n",
        "        worksheet.update_cell(index + 2, summary_col_index, summary_str)\n",
        "\n",
        "print(\"要約の更新が完了しました。\")\n",
        "print(qa_data[['質問', '要約']].head()) # 更新されたデータの確認"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f1OgMf05OgH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "cLfJNVImBx9R"
      ],
      "authorship_tag": "ABX9TyMqUFX/m+1t3juE4g9IROt8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}